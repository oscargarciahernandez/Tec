{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterNotebook de la App para Electrobuzz\n",
    "\n",
    "## Intro\n",
    "\n",
    "En este cuaderno se pretende ir añadiendo todos los avances relativos al proyecto del desarrollo de la App para la descarga de música mediante Electrobuzz-Cosmobox. Cabe mencionar que este proyecto no está pensado para realizar un web-scrapping ilícito. Lo que se pretende mediante este programa es agilizar el proceso de descarga de música de Electrobuzz, pero teniendo en cuenta que es necesario contar con una cuenta de Cosmobox para ello. Tambien se pretende aprender a programar un scraper en python y hacer una pequeña interfaz de usuario que pueda acabar en convertirse en un ejecutable, QUIERO APRENDER A HACER SOFTWARE, PERO SOFTWARE CACHARRO. \n",
    "\n",
    "## Fundamentos\n",
    "\n",
    "En principio esta app se basará en realizar peticiones mediante el paquete requests. Se ha planteado realizar el trabajo inicial en 3 rondas de peticiones. \n",
    "\n",
    "En primera instancia todos las urls de todos los albumes disponibles en Electrobuzz. \n",
    "Una vez se consiga tener un listado de todos los links se pretende realizar una seguna ronda de requests que se encarge de completar la información de cada album, asi como Artista, Remixers, tamaño de la descarga y SOBRE TODO comprobar que exista el link de descarga de Cosmobox (existen albumes que no estan disponibles para descargar en Cosmobox y por lo tanto no son accesibles para nosotros). \n",
    "En la tercera ronda de requests se pretende comprobar que el link de Cosmobox no está caído (Hay  bastantes links que no funcionan).\n",
    "\n",
    "Con toda esta informacion se pasará a crear una interfaz empleando el paquete tkinter, la cual debe contar con un motor de busqueda dentro de toda la información de electrobuzz que nos permita rápidamente seleccionar los albumes que se quieren descargar. Sería interensante que dentro de este motor de busqueda se pudiera filtrar por nombre, sello, fecha, etc, para agilizar la labor de buscar la mñusica que nos apetezca. \n",
    "\n",
    "Dentro de la interfaz de tkinter habrá un botón para actualizar la información contenida en el software, como un método para tener al día todos los realeases de la página... Aunque tambien se podría realizar una actualización de manera automática. \n",
    "\n",
    "Debe ser sencillo selecionar la música que queremos descargar, igual añadimos un click marker y un botón de descarga al final de la página. Ya veré. Este paso no se si se realizará con requests (espero que si) o con Selenium... ambos tienen sus ventajas y desventajas. \n",
    "\n",
    "\n",
    "A continuación voy a añadir las librerias que voy empleando. Destacando requests y BeautifulSoup. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas detectados\n",
    "\n",
    "El primero y yo creo que más importante es que los cabrones de Electrobuzz no dejan realizar scraping... Esto se puede confirmar viendo el archivo robot.txt de la página web. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Robot.txt de la página Electrobuzz](img_jupyter/robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El asterísco en User-Agent siginifica que no se permiten spyders, crawlers y compañia. Por lo tanto intentarán bloquear cualquier comportamiento que sea similar al de un boot, por ejemplo, muchas requests en poco tiempo, o muchas requests iguales. Una vez han detectado a un boot lo siguiente que hacen es bloquear la IP que está realizando las peticiones. La IP la marca tu router, pa entendernos, por lo tanto hay que usar VPN o proxies. Investigando un poquito llegue a la conclusión de que iba a ser más fácil incluir el método de cambiar de proxies, sobre todo porque el paquete requests ya incluye métodos para fijar una proxie. \n",
    "\n",
    "## Buscar y cambiar proxy\n",
    "\n",
    "Para esto es necesario saber que las proxies no son fijas, es decir, las proxies son puntos de conexion que no siempre se encuentran disponibles por ello es necesario ir actualizando la lista de proxies a las que vamos a conectarnos. Para ello yo voy a usar una página web que ofrece un servicio de proxies gratuitas. \n",
    "\n",
    "Creo una función que devuelve las proxies encontradas en la página web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ip': u'186.225.97.246', 'port': u'43082'},\n",
       " {'ip': u'170.238.41.16', 'port': u'31208'},\n",
       " {'ip': u'125.25.165.97', 'port': u'47106'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_proxies():   \n",
    "   \n",
    "  proxies_req = requests.get('https://www.sslproxies.org/')\n",
    "\n",
    "\n",
    "  soup = BeautifulSoup(proxies_req.content, 'html.parser')\n",
    "  proxies_table = soup.find(id='proxylisttable')\n",
    "  \n",
    "  proxies=[]\n",
    "\n",
    "  # Save proxies in the array\n",
    "  for row in proxies_table.tbody.find_all('tr'):\n",
    "    proxies.append({\n",
    "      'ip':   row.find_all('td')[0].string,\n",
    "      'port': row.find_all('td')[1].string\n",
    "    })\n",
    "\n",
    "  return proxies\n",
    "\n",
    "\n",
    "\n",
    "get_proxies() [:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario entender como se deben incluir las proxies a las requests. Sin más, te lo demuestro con la función que he creado para poner en formato las proxies par que las entienda el paquete request. \n",
    "\n",
    "A esta función hay que meter como argumentos la lista de proxies que devuelte get_proxies() y el número dentro de la lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http': 'http://200.255.122.174:8080',\n",
       " 'https': 'https://200.255.122.174:8080'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_proxi_for_req(proxies,n):\n",
    "    proxi_http='http://' + str(proxies[n]['ip'])+ ':' +proxies[n]['port']\n",
    "    proxi_https='https://' + str(proxies[n]['ip'])+ ':' +proxies[n]['port']\n",
    "    proxi1 = {'http': str(proxi_http), \"https\": str(proxi_https)}\n",
    "    return proxi1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "proxies=get_proxies()\n",
    "\n",
    "set_proxi_for_req(proxies, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ese es el formato que requiere el paquete requests para fijar las proxies y se introduce la siguiente manera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests.get('url', proxies= 'proxi en formato visto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosotras lo vamos a utilizar asi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.get('url', proxies= set_proxi_for_req(proxies,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemática de cambiar el proxy\n",
    "\n",
    "Resulta que se las saben todas... cuando cambias el proxy muchas veces o algo así te aparece un capcha de \"I'm not a robot\" y eso jode que flipas, sobre todo cuando no eres el dios de la programación. He visto por hay que hay peña que es capaz de resolver capchas con python pero nunca con request... Por eso he decidido olvidarme de cambiar proxies y movidas. \n",
    "\n",
    "## Plan C: User Agents\n",
    "\n",
    "Resulta que uno de los metodos para evitar ser baneado en páginas webs y esas cosas es cambiar el Agent-user es decir, si ven que hay muchas peticiones provenientes del mismo usuario pues te joden y te banean. \n",
    "\n",
    "En principio parece que funciona bien, tampoco quiero adelantar acontecimientos... Pero de momento voy plasmando el código que he hecho para obtener una lista de mogollón de Agent users. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'Mozilla/5.0 (compatible; U; ABrowse 0.6;  Syllable) AppleWebKit/420+ (KHTML, like Gecko)'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DESCARGAR PEDAZO DE LISTA DE USER AGENTS        \n",
    "def get_user_agents():\n",
    "    user_agents= requests.get('http://www.useragentstring.com/pages/useragentstring.php?typ=Browser')\n",
    "    user_soup = BeautifulSoup(user_agents.content, 'html.parser')\n",
    "    user_soup1=user_soup.find_all('ul')\n",
    "    \n",
    "    users_list=[]\n",
    "    for i in np.arange(0,len(user_soup1)):\n",
    "        user_soup2= user_soup1[i].find_all('a')\n",
    "        for j in np.arange(0,len(user_soup2)):\n",
    "            users_list.append(user_soup2[j].text)\n",
    "            \n",
    "    return users_list\n",
    "\n",
    "\n",
    "\n",
    "#PONER USER AGENTS EN FORMATO PARA HEADERS\n",
    "def set_user_agents(users_list, n):\n",
    "    headers = {'User-Agent': str(users_list[n])}\n",
    "    return headers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "users_list= get_user_agents()\n",
    "\n",
    "set_user_agents(users_list, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tambien hago una funcion para poner en el formato que a requests le gusta. La request se haría de la siguiente manera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.get('url',headers='aqui el user agent en formato')\n",
    "\n",
    "#requests.get('url',headers=set_user_agents(users_list,random.randrange(0,len(users_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping como tal\n",
    "\n",
    "Pos na, en lo que me como la cabeza pa poder hacer un chorro de request sin el problema de ser baneado paso a poner el código que uso para el scrap de la web como tal.\n",
    "\n",
    "\n",
    "A continuación muestro el scrap de la página principal, principalmente porque tiene un detallito diferente al resto de páginas, pero tambien para obtener el númeor de páginas totales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_vector_pags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-dc0cbacc048f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m##vector de pags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtotal_pags_vec\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_vector_pags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#Info de la página principal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_vector_pags' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "users_list= get_user_agents()        \n",
    "\n",
    "\n",
    "first_req= requests.get('https://www.electrobuzz.net/',\n",
    "                        headers=set_user_agents(users_list,\n",
    "                                                 random.randrange(0, \n",
    "                                                                  len(users_list))))\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(first_req.content, 'html.parser')\n",
    "soup1=soup.find_all('div', \n",
    "                    {'class': 'listing listing-blog listing-blog-1 clearfix columns-1 columns-1'})\n",
    "soup2=soup1[0].find_all('article')\n",
    "\n",
    "\n",
    "##vector de pags\n",
    "total_pags_vec= get_vector_pags(soup)\n",
    "\n",
    "#Info de la página principal\n",
    "info_pag_1= obtn_link_title_img(soup2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para la obtencion de la info de la primera pagina se usa la misma funcion que para el scrap del resto de páginas. Esta funcion crea un lista por album con 3 cosas: LINK, NOMBRE E IMAGEN. notese que la imagen se guarda por defecto en path que yo he especificado a mano, pero seria interesante que este path se seteara de una manera más guay... nosecomolaverdad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtn_link_title_img(bs4_element):\n",
    "    list_link_title= []\n",
    "    for i in np.arange(0,len(soup2)):\n",
    "        title= soup2[i].find('a')['title']\n",
    "        link= soup2[i].find('a')['href']\n",
    "        \n",
    "        \n",
    "        \n",
    "        link_img= soup2[0].find('a')['data-src']\n",
    "        ext=link_img[-3:]\n",
    "        filename_0=''.join(e for e in title if e.isalnum())\n",
    "        filename= filename_0 + '.' + ext\n",
    "        path='C:/Users/Oscar/Desktop/img/'\n",
    "        file_ext= path + filename\n",
    "        \n",
    "        r = requests.get(link_img, stream=True)\n",
    "        \n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            with open(file_ext, 'wb') as f:\n",
    "                r.raw.decode_content = True\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "                \n",
    "        if os.path.isfile(file_ext):\n",
    "            link_title= list([link,title,file_ext])\n",
    "            list_link_title.append(link_title)\n",
    "        else: \n",
    "            link_title= list([link, title,str('NO')])\n",
    "            list_link_title.append(link_title)\n",
    "            \n",
    "    return list_link_title\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A este código hay que sumarle la siguiente funcion para realizar un scrap total. \n",
    "\n",
    "\n",
    "A continuacion el funcion que se emplearía en caso de que la idea de los proxis funcionará aunque parece que no va a ser asi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srch_info(vector_paginas, proxi1):\n",
    "    \n",
    "    url_base= 'https://www.electrobuzz.net/page'\n",
    "        \n",
    "    list_info=[]\n",
    "    for i in vector_paginas:\n",
    "        url_page= url_base + '/' + str(i)+ '/'\n",
    "        \n",
    "        req= requests.get(url_page, proxies= proxi1)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        soup2=soup.find_all('article')\n",
    "    \n",
    "        \n",
    "        list_info.append(obtn_link_title_img(soup2))\n",
    "        \n",
    "    \n",
    "    return list_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Y aquí la función que espero que sea definitiva, a esta funcion se le mete como argumentos el vector páginas, que es sin más un vector numérico desde 2 hasta la ultima página y ua que es la lista que hemos citado anteriormente de User-Agents. \n",
    "\n",
    "es guay porque yo con el vector páginas puedo elegir hasta donde o desde donde empiezo a descargar... incluso puedo meter páginas específicas aunque eso no tiene mucho sentido pero poder puedo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srch_info_chg_ua(vector_paginas, ua):\n",
    "    \n",
    "    url_base= 'https://www.electrobuzz.net/page'\n",
    "        \n",
    "    list_info=[]\n",
    "    for i in vector_paginas:\n",
    "        url_page= url_base + '/' + str(i)+ '/'\n",
    "        headers=set_user_agents(ua,random.randrange(0,len(ua)))\n",
    "        \n",
    "        req= requests.get(url_page, headers= headers)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        soup2=soup.find_all('article')\n",
    "    \n",
    "        \n",
    "        list_info.append(obtn_link_title_img(soup2))\n",
    "        \n",
    "    \n",
    "    return list_info\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
